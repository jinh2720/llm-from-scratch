{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rickiepark/llm-from-scratch/blob/main/ch05/01_main-chapter-code/exercise-solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e984a25",
      "metadata": {
        "id": "4e984a25"
      },
      "source": [
        "<table style=\"width:100%\">\n",
        "<tr>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<font size=\"2\">\n",
        "세바스찬 라시카(Sebastian Raschka)가 쓴 <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a>의 번역서 <br><<b><a href=\"<a href=\"http://tensorflow.blog/llm-from-scratch\">밑바닥부터 만들면서 배우는 LLM</a></b>>의 예제 코드입니다.<br>\n",
        "<br>코드 저장소: <a href=\"https://github.com/rickiepark/llm-from-scratch\">https://github.com/rickiepark/llm-from-scratch</a>\n",
        "</font>\n",
        "</td>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<a href=\"http://tensorflow.blog/llm-from-scratch\"><img src=\"https://tensorflowkorea.wordpress.com/wp-content/uploads/2025/09/ebb091ebb094eb8ba5llm_ebb3b8ecb185_ec959eeba9b4.jpg\" width=\"100px\"></a>\n",
        "</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "338c1708",
      "metadata": {
        "id": "338c1708"
      },
      "source": [
        "# 5장 연습문제 솔루션\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73809619",
      "metadata": {
        "id": "73809619",
        "outputId": "705141dc-d6bc-4fd5-a385-1f525b234724",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "numpy 버전: 2.0.2\n",
            "tiktoken 버전: 0.12.0\n",
            "torch 버전: 2.8.0+cu126\n",
            "tensorflow 버전: 2.19.0\n"
          ]
        }
      ],
      "source": [
        "from importlib.metadata import version\n",
        "\n",
        "pkgs = [\"numpy\",\n",
        "        \"tiktoken\",\n",
        "        \"torch\",\n",
        "        \"tensorflow\" # OpenAI의 사전 훈련된 가중치를 위해\n",
        "       ]\n",
        "for p in pkgs:\n",
        "    print(f\"{p} 버전: {version(p)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4071ab31",
      "metadata": {
        "id": "4071ab31"
      },
      "source": [
        "# 연습문제 5.1: 온도 스케일링이 적용된 소프트맥스 점수 및 샘플링 확률\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "374eddcb",
      "metadata": {
        "id": "374eddcb"
      },
      "source": [
        "- 이 절에서 정의한 `print_sampled_tokens` 함수를 사용하여 \"pizza\" 단어가 샘플링된 횟수를 출력할 수 있습니다.\n",
        "- 5.3.1 섹션에서 정의한 코드부터 시작하겠습니다.\n",
        "\n",
        "- 온도가 0 또는 0.1인 경우 0회 샘플링되고, 온도가 5이면 32회 샘플링됩니다. 추정 확률은 32/1000 * 100% = 3.2%입니다.\n",
        "\n",
        "- 실제 확률은 4.3%이며, 스케일이 조정된 소프트맥스 확률 텐서(`scaled_probas[2][6]`)에 포함되어 있습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb0f802a",
      "metadata": {
        "id": "fb0f802a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "vocab = {\n",
        "    \"closer\": 0,\n",
        "    \"every\": 1,\n",
        "    \"effort\": 2,\n",
        "    \"forward\": 3,\n",
        "    \"inches\": 4,\n",
        "    \"moves\": 5,\n",
        "    \"pizza\": 6,\n",
        "    \"toward\": 7,\n",
        "    \"you\": 8,\n",
        "}\n",
        "inverse_vocab = {v: k for k, v in vocab.items()}\n",
        "\n",
        "next_token_logits = torch.tensor(\n",
        "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
        ")\n",
        "\n",
        "def print_sampled_tokens(probas):\n",
        "    torch.manual_seed(123)\n",
        "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
        "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
        "    for i, freq in enumerate(sampled_ids):\n",
        "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
        "\n",
        "def softmax_with_temperature(logits, temperature):\n",
        "    scaled_logits = logits / temperature\n",
        "    return torch.softmax(scaled_logits, dim=0)\n",
        "\n",
        "\n",
        "temperatures = [1, 0.1, 5]  # 원본, 높은, 그리고 낮은 온도\n",
        "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28b9eab0",
      "metadata": {
        "id": "28b9eab0"
      },
      "source": [
        "- 이제 `scaled_probas`를 반복하면서 샘플링 빈도를 출력할 수 있습니다:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30b04f31",
      "metadata": {
        "id": "30b04f31",
        "outputId": "53951c32-6d6b-4754-e798-aedffdb0dce7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "온도: 1\n",
            "73 x closer\n",
            "0 x every\n",
            "0 x effort\n",
            "582 x forward\n",
            "2 x inches\n",
            "0 x moves\n",
            "0 x pizza\n",
            "343 x toward\n",
            "\n",
            "\n",
            "온도: 0.1\n",
            "0 x closer\n",
            "0 x every\n",
            "0 x effort\n",
            "985 x forward\n",
            "0 x inches\n",
            "0 x moves\n",
            "0 x pizza\n",
            "15 x toward\n",
            "\n",
            "\n",
            "온도: 5\n",
            "165 x closer\n",
            "75 x every\n",
            "42 x effort\n",
            "239 x forward\n",
            "71 x inches\n",
            "46 x moves\n",
            "32 x pizza\n",
            "227 x toward\n",
            "103 x you\n"
          ]
        }
      ],
      "source": [
        "for i, probas in enumerate(scaled_probas):\n",
        "    print(\"\\n\\n온도:\", temperatures[i])\n",
        "    print_sampled_tokens(probas)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c526e6d",
      "metadata": {
        "id": "2c526e6d"
      },
      "source": [
        "- \"pizza\"라는 단어가 샘플링될 때 샘플링은 실제 확률의 근사치를 제공한다는 점에 유의하세요.\n",
        "- 예: 1000번 중 32번 샘플링된 경우 추정 확률은 3.2%입니다.\n",
        "- 실제 확률을 얻으려면 `scaled_probas`에서 해당 항목에 직접 액세스하여 확률을 확인할 수 있습니다.\n",
        "\n",
        "- \"pizza\"는 어휘에서 7번째 항목이므로 온도 5의 경우 다음과 같이 얻습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e121228e",
      "metadata": {
        "id": "e121228e",
        "outputId": "79dae3ce-a572-468d-c89b-80b09325e6c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0430)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "temp5_idx = 2\n",
        "pizza_idx = 6\n",
        "\n",
        "scaled_probas[temp5_idx][pizza_idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1245a380",
      "metadata": {
        "id": "1245a380"
      },
      "source": [
        "온도가 5로 설정된 경우 \"pizza\"라는 단어가 샘플링될 확률은 4.3%입니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4e3e415",
      "metadata": {
        "id": "d4e3e415"
      },
      "source": [
        "# 연습문제 5.2: 다양한 온도 및 top-k 설정"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96a322ef",
      "metadata": {
        "id": "96a322ef"
      },
      "source": [
        "- 온도 및 top-k 설정 모두 개별 LLM에 따라 조정해야 합니다 (원하는 출력을 생성할 때까지 시행착오 과정).\n",
        "- 바람직한 결과는 애플리케이션마다 다릅니다.\n",
        "  - 낮은 top-k 및 온도는 교육 콘텐츠, 기술 문서 또는 질문 응답, 데이터 분석, 코드 생성 등을 작성할 때 바람직한 덜 무작위적인 결과를 생성합니다.\n",
        "  - 높은 top-k 및 온도는 브레인스토밍 작업, 창의적인 글쓰기 등에 더 바람직한 더 다양하고 무작위적인 출력을 생성합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2356354",
      "metadata": {
        "id": "e2356354"
      },
      "source": [
        "# 연습문제 5.3: 디코딩 함수의 결정론적인 동작\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "785ea94e",
      "metadata": {
        "id": "785ea94e"
      },
      "source": [
        "`generate` 함수에서 결정론적인 동작을 강제하는 방법은 여러 가지가 있습니다:\n",
        "\n",
        "1. `temperature=0.0`으로 설정;\n",
        "2. `top_k=1`로 설정.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 깃허브에서 previous_chapters.py 파일을 다운로드합니다.\n",
        "!wget https://bit.ly/3HlFmc8 -O previous_chapters.py"
      ],
      "metadata": {
        "id": "ThHC6d1LPk08",
        "outputId": "c7f1d33d-b410-49fb-fafb-9f3254a196b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ThHC6d1LPk08",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-23 03:40:11--  https://bit.ly/3HlFmc8\n",
            "Resolving bit.ly (bit.ly)... 67.199.248.11, 67.199.248.10\n",
            "Connecting to bit.ly (bit.ly)|67.199.248.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://raw.githubusercontent.com/rickiepark/llm-from-scratch/refs/heads/main/ch05/01_main-chapter-code/previous_chapters.py [following]\n",
            "--2025-10-23 03:40:11--  https://raw.githubusercontent.com/rickiepark/llm-from-scratch/refs/heads/main/ch05/01_main-chapter-code/previous_chapters.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9905 (9.7K) [text/plain]\n",
            "Saving to: ‘previous_chapters.py’\n",
            "\n",
            "previous_chapters.p 100%[===================>]   9.67K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-10-23 03:40:11 (121 MB/s) - ‘previous_chapters.py’ saved [9905/9905]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 구글 드라이브에서 5장에서 훈련한 모델 가중치(model.pth)를 다운로드합니다.\n",
        "!gdown 1Lze7x_4Bd3Sd22sZrCG7cxKOE2wcMKrh"
      ],
      "metadata": {
        "id": "dMgiGwAYSJE_",
        "outputId": "5d930f0d-0a06-4e5f-c155-6bb4d6a60d9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "dMgiGwAYSJE_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1Lze7x_4Bd3Sd22sZrCG7cxKOE2wcMKrh\n",
            "From (redirected): https://drive.google.com/uc?id=1Lze7x_4Bd3Sd22sZrCG7cxKOE2wcMKrh&confirm=t&uuid=ccf1f7b5-55af-4332-af72-c172721d72fd\n",
            "To: /content/model.pth\n",
            "100% 653M/653M [00:03<00:00, 180MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d43a66cc",
      "metadata": {
        "id": "d43a66cc"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "import torch\n",
        "from previous_chapters import GPTModel\n",
        "\n",
        "\n",
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,    # 어휘 크기\n",
        "    \"context_length\": 256,  # 짧아진 컨텍스트 길이 (원래: 1024)\n",
        "    \"emb_dim\": 768,         # 임베딩 차원\n",
        "    \"n_heads\": 12,          # 어텐션 헤드 수\n",
        "    \"n_layers\": 12,         # 층 수\n",
        "    \"drop_rate\": 0.1,       # 드롭아웃 비율\n",
        "    \"qkv_bias\": False       # 쿼리-키-값 편향\n",
        "}\n",
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))\n",
        "model.eval();"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 깃허브에서 gpt_generate.py 파일을 다운로드합니다.\n",
        "!wget https://bit.ly/3FzTX37 -O gpt_generate.py"
      ],
      "metadata": {
        "id": "cfO1y1e0SkuF",
        "outputId": "9552bc22-b776-4a17-e7d9-29f9aa1e03c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "cfO1y1e0SkuF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-23 03:40:24--  https://bit.ly/3FzTX37\n",
            "Resolving bit.ly (bit.ly)... 67.199.248.11, 67.199.248.10\n",
            "Connecting to bit.ly (bit.ly)|67.199.248.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://raw.githubusercontent.com/rickiepark/llm-from-scratch/refs/heads/main/ch05/01_main-chapter-code/gpt_generate.py [following]\n",
            "--2025-10-23 03:40:25--  https://raw.githubusercontent.com/rickiepark/llm-from-scratch/refs/heads/main/ch05/01_main-chapter-code/gpt_generate.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12576 (12K) [text/plain]\n",
            "Saving to: ‘gpt_generate.py’\n",
            "\n",
            "gpt_generate.py     100%[===================>]  12.28K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-10-23 03:40:25 (118 MB/s) - ‘gpt_generate.py’ saved [12576/12576]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35e10566",
      "metadata": {
        "id": "35e10566"
      },
      "outputs": [],
      "source": [
        "from gpt_generate import generate, text_to_token_ids, token_ids_to_text\n",
        "from previous_chapters import generate_text_simple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0dad81f",
      "metadata": {
        "id": "d0dad81f",
        "outputId": "c25897ed-0bfe-495b-8349-fa0a3d1f4117",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "출력 텍스트:\n",
            " Every effort moves you?\"\n",
            "\n",
            "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# torch.argmax를 사용하는 결정론적 함수\n",
        "\n",
        "start_context = \"Every effort moves you\"\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(start_context, tokenizer),\n",
        "    max_new_tokens=25,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(\"출력 텍스트:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c888f0bf",
      "metadata": {
        "id": "c888f0bf",
        "outputId": "9432bbf2-c44a-4298-d5d7-a9978039d7f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "출력 텍스트:\n",
            " Every effort moves you?\"\n",
            "\n",
            "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 결정론적 동작: top_k 없음, 온도 스케일링 없음\n",
        "\n",
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
        "    max_new_tokens=25,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
        "    top_k=None,\n",
        "    temperature=0.0\n",
        ")\n",
        "\n",
        "print(\"출력 텍스트:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "979a042f",
      "metadata": {
        "id": "979a042f"
      },
      "source": [
        "- 이전 코드 셀을 다시 실행하면 똑같은 텍스트가 생성됩니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bbfa62f",
      "metadata": {
        "id": "5bbfa62f",
        "outputId": "1a651e0e-cd31-4034-ff86-0370ad1e08f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "출력 텍스트:\n",
            " Every effort moves you?\"\n",
            "\n",
            "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 결정론적 동작: top_k 없음, 온도 스케일링 없음\n",
        "\n",
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
        "    max_new_tokens=25,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
        "    top_k=None,\n",
        "    temperature=0.0\n",
        ")\n",
        "\n",
        "print(\"출력 텍스트:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f677573",
      "metadata": {
        "id": "5f677573"
      },
      "source": [
        "# 연습문제 5.4: 사전 훈련 계속 수행\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8bd77d9",
      "metadata": {
        "id": "d8bd77d9"
      },
      "source": [
        "- 5장에서 모델을 처음 훈련했던 파이썬 세션에 아직 있다면, 사전 훈련을 한 에포크 더 계속하려면 이 장의 노트북에서 저장한 모델과 옵티마이저를 로드하고 `train_model_simple` 함수를 다시 호출하면 됩니다.\n",
        "\n",
        "- 이 새로운 코드 환경에서 재현가능하도록 만들려면 몇 가지 단계가 더 필요합니다.\n",
        "- 먼저 토크나이저, 모델 및 옵티마이저를 로드합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 구글 드라이브에서 5장에서 훈련한 모델 가중치(model_and_optimizer.pth)를 다운로드합니다.\n",
        "!gdown 1D_q35z6cqjJTuB3RlD8X7MFmZPF5NI1q"
      ],
      "metadata": {
        "id": "vdZiffhcUsYD",
        "outputId": "7e0e2594-9bfa-4894-8131-a92fa5442d01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "vdZiffhcUsYD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1D_q35z6cqjJTuB3RlD8X7MFmZPF5NI1q\n",
            "From (redirected): https://drive.google.com/uc?id=1D_q35z6cqjJTuB3RlD8X7MFmZPF5NI1q&confirm=t&uuid=14ea787f-c127-4e3b-9236-a004c8cb87b9\n",
            "To: /content/model_and_optimizer.pth\n",
            "100% 1.95G/1.95G [00:23<00:00, 82.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76d3d74f",
      "metadata": {
        "id": "76d3d74f"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "import torch\n",
        "from previous_chapters import GPTModel\n",
        "\n",
        "\n",
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,   # 어휘 크기\n",
        "    \"context_length\": 256, # 짧아진 컨텍스트 길이 (원래: 1024)\n",
        "    \"emb_dim\": 768,        # 임베딩 차원\n",
        "    \"n_heads\": 12,         # 어텐션 헤드 수\n",
        "    \"n_layers\": 12,        # 층 수\n",
        "    \"drop_rate\": 0.1,      # 드롭아웃 비율\n",
        "    \"qkv_bias\": False      # 쿼리-키-값 편향\n",
        "}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "checkpoint = torch.load(\"model_and_optimizer.pth\", weights_only=True)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "model.train();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5768491",
      "metadata": {
        "id": "f5768491"
      },
      "source": [
        "- 다음으로, 데이터 로더를 초기화합니다:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa3b1ba3",
      "metadata": {
        "id": "fa3b1ba3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import request\n",
        "from previous_chapters import create_dataloader_v1\n",
        "\n",
        "\n",
        "file_path = \"the-verdict.txt\"\n",
        "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    response = requests.get(url, timeout=30)\n",
        "    response.raise_for_status()\n",
        "    text_data = response.text\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(text_data)\n",
        "else:\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        text_data = file.read()\n",
        "\n",
        "# 책에서는 다음 코드를 사용했지만 VPN을 사용하는 경우 urllib가 문제를 일으킬 수 있습니다.\n",
        "# 따라서 더 안정적인 `requests` 패키지를 사용합니다.\n",
        "\n",
        "\"\"\"\n",
        "import urllib.request\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    with urllib.request.urlopen(url) as response:\n",
        "        text_data = response.read().decode('utf-8')\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(text_data)\n",
        "else:\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        text_data = file.read()\n",
        "\"\"\"\n",
        "\n",
        "# 훈련/검증 비율\n",
        "train_ratio = 0.90\n",
        "split_idx = int(train_ratio * len(text_data))\n",
        "train_data = text_data[:split_idx]\n",
        "val_data = text_data[split_idx:]\n",
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = create_dataloader_v1(\n",
        "    train_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=True,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_loader = create_dataloader_v1(\n",
        "    val_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=False,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c38d094",
      "metadata": {
        "id": "6c38d094"
      },
      "source": [
        "- 마지막으로, `train_model_simple` 함수를 사용하여 모델을 학습시킵니다:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 깃허브에서 gpt_train.py 파일을 다운로드합니다.\n",
        "!wget https://bit.ly/4mODn07 -O gpt_train.py"
      ],
      "metadata": {
        "id": "cxu4lv0JWB_a",
        "outputId": "4caf8ad5-48f1-46fc-e748-44cc927c3fc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "cxu4lv0JWB_a",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-23 03:41:09--  https://bit.ly/4mODn07\n",
            "Resolving bit.ly (bit.ly)... 67.199.248.11, 67.199.248.10\n",
            "Connecting to bit.ly (bit.ly)|67.199.248.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://raw.githubusercontent.com/rickiepark/llm-from-scratch/refs/heads/main/ch05/01_main-chapter-code/gpt_train.py [following]\n",
            "--2025-10-23 03:41:10--  https://raw.githubusercontent.com/rickiepark/llm-from-scratch/refs/heads/main/ch05/01_main-chapter-code/gpt_train.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8360 (8.2K) [text/plain]\n",
            "Saving to: ‘gpt_train.py’\n",
            "\n",
            "gpt_train.py        100%[===================>]   8.16K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-10-23 03:41:10 (84.9 MB/s) - ‘gpt_train.py’ saved [8360/8360]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f33e5b04",
      "metadata": {
        "id": "f33e5b04",
        "outputId": "05bf0b9d-cda3-4c03-ef2c-11e30d6accb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 0.665, Val loss 6.365\n",
            "Ep 1 (Step 000005): Train loss 0.480, Val loss 6.437\n",
            "Every effort moves you?\" \"Oh, my hostess was \"interesting\": on that point I could have given Miss Croft the fullest reassurance. It was just because she was _not_ interesting--if I saw that, my eye fell on a small picture\n"
          ]
        }
      ],
      "source": [
        "from gpt_train import train_model_simple\n",
        "\n",
        "num_epochs = 1\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
        "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffc33dca",
      "metadata": {
        "id": "ffc33dca"
      },
      "source": [
        "# 연습문제 5.5: 사전 훈련된 모델의 훈련 및 검증 세트 손실\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4624b41",
      "metadata": {
        "id": "a4624b41"
      },
      "source": [
        "- 다음 코드를 사용하여 GPT 모델의 훈련 및 검증 세트 손실을 계산할 수 있습니다:\n",
        "\n",
        "```python\n",
        "train_loss = calc_loss_loader(train_loader, gpt, device)\n",
        "val_loss = calc_loss_loader(val_loader, gpt, device)\n",
        "```\n",
        "\n",
        "- 124M 매개변수에 대한 결과 손실은 다음과 같습니다:\n",
        "\n",
        "```\n",
        "Training loss: 3.754748503367106\n",
        "Validation loss: 3.559617757797241\n",
        "```\n",
        "\n",
        "- 주요 관찰 사항은 학습 세트와 검증 세트의 성능이 비슷하다는 것입니다.\n",
        "- 이는 여러 가지 이유로 설명될 수 있습니다:\n",
        "\n",
        "1. OpenAI가 GPT-2를 훈련시킬 때 The Verdict는 사전 훈련 데이터 세트에 포함되지 않았습니다. 따라서 모델은 훈련 세트에 명시적으로 과적합되지 않으며 The Verdict의 훈련 및 검증 세트 부분에서 유사하게 잘 수행됩니다. (검증 세트 손실이 훈련 세트 손실보다 약간 낮은데, 이는 딥러닝에서는 드문 현상입니다. 하지만 데이터 세트가 비교적 작기 때문에 무작위 잡음 때문일 가능성이 큽니다. 실제로 과대적합이 없다면 훈련 세트와 검증 세트의 성능은 거의 동일할 것으로 예상됩니다).\n",
        "\n",
        "2. The Verdict가 GPT-2의 훈련 데이터셋에 포함되었습니다. 이 경우 검증 세트도 학습에 사용되었을 것이기 때문에 모델이 훈련 데이터에 과대적합되었는지 여부를 알 수 없습니다. 과대적합 정도를 평가하려면 OpenAI가 GPT-2 학습을 완료한 후 생성된 새로운 데이터셋가 필요합니다. 이는 사전 학습에 사용되지 않았음을 확인하기 위한 것입니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6ba5dbb",
      "metadata": {
        "id": "f6ba5dbb"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "import torch\n",
        "from previous_chapters import GPTModel\n",
        "\n",
        "\n",
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,   # 어휘 크기\n",
        "    \"context_length\": 256, # 짧아진 컨텍스트 길이 (원래: 1024)\n",
        "    \"emb_dim\": 768,        # 임베딩 차원\n",
        "    \"n_heads\": 12,         # 어텐션 헤드 수\n",
        "    \"n_layers\": 12,        # 레이어 수\n",
        "    \"drop_rate\": 0.1,      # 드롭아웃 비율\n",
        "    \"qkv_bias\": False      # 쿼리-키-값 편향\n",
        "}\n",
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 깃허브에서 gpt_download.py 파일을 다운로드합니다.\n",
        "!wget https://bit.ly/4kSEn1v -O gpt_download.py"
      ],
      "metadata": {
        "id": "5JVXoqc2W0Cb",
        "outputId": "4f37e87d-a7b0-49e2-b448-721f46862829",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "5JVXoqc2W0Cb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-23 03:41:12--  https://bit.ly/4kSEn1v\n",
            "Resolving bit.ly (bit.ly)... 67.199.248.11, 67.199.248.10\n",
            "Connecting to bit.ly (bit.ly)|67.199.248.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://raw.githubusercontent.com/rickiepark/llm-from-scratch/refs/heads/main/ch05/01_main-chapter-code/gpt_download.py [following]\n",
            "--2025-10-23 03:41:12--  https://raw.githubusercontent.com/rickiepark/llm-from-scratch/refs/heads/main/ch05/01_main-chapter-code/gpt_download.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6333 (6.2K) [text/plain]\n",
            "Saving to: ‘gpt_download.py’\n",
            "\n",
            "gpt_download.py     100%[===================>]   6.18K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-10-23 03:41:13 (68.3 MB/s) - ‘gpt_download.py’ saved [6333/6333]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "691519b7",
      "metadata": {
        "id": "691519b7",
        "outputId": "24e85579-4ba6-4978-f07b-36b0b54e8618",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 174kiB/s]\n",
            "encoder.json: 100%|██████████| 1.04M/1.04M [00:01<00:00, 581kiB/s]\n",
            "hparams.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 187kiB/s]\n",
            "model.ckpt.data-00000-of-00001: 100%|██████████| 498M/498M [04:09<00:00, 1.99MiB/s]\n",
            "model.ckpt.index: 100%|██████████| 5.21k/5.21k [00:00<00:00, 9.17MiB/s]\n",
            "model.ckpt.meta: 100%|██████████| 471k/471k [00:01<00:00, 338kiB/s]\n",
            "vocab.bpe: 100%|██████████| 456k/456k [00:01<00:00, 367kiB/s]\n"
          ]
        }
      ],
      "source": [
        "from gpt_download import download_and_load_gpt2\n",
        "\n",
        "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dbbb9c8",
      "metadata": {
        "id": "0dbbb9c8"
      },
      "outputs": [],
      "source": [
        "# 모델 구성을 간결하게 딕셔너리로 정의합니다.\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "# 기본 구성을 복사하고 특정 모델 설정으로 업데이트합니다.\n",
        "model_name = \"gpt2-small (124M)\"  # 예시 모델 이름\n",
        "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
        "NEW_CONFIG.update(model_configs[model_name])\n",
        "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
        "\n",
        "gpt = GPTModel(NEW_CONFIG)\n",
        "gpt.eval();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8823b3bd",
      "metadata": {
        "id": "8823b3bd"
      },
      "outputs": [],
      "source": [
        "from gpt_generate import load_weights_into_gpt\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "load_weights_into_gpt(gpt, params)\n",
        "gpt.to(device);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f95cf8aa",
      "metadata": {
        "id": "f95cf8aa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "from previous_chapters import create_dataloader_v1\n",
        "\n",
        "\n",
        "file_path = \"the-verdict.txt\"\n",
        "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    with urllib.request.urlopen(url) as response:\n",
        "        text_data = response.read().decode('utf-8')\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(text_data)\n",
        "else:\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        text_data = file.read()\n",
        "\n",
        "\n",
        "# 훈련/검증 비율\n",
        "train_ratio = 0.90\n",
        "split_idx = int(train_ratio * len(text_data))\n",
        "train_data = text_data[:split_idx]\n",
        "val_data = text_data[split_idx:]\n",
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = create_dataloader_v1(\n",
        "    train_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=True,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_loader = create_dataloader_v1(\n",
        "    val_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=False,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8eda7dac",
      "metadata": {
        "id": "8eda7dac",
        "outputId": "80630676-ddfe-4781-82b7-5b3ce7782b53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 손실: 3.7547631528642444\n",
            "검증 손실: 3.559633731842041\n"
          ]
        }
      ],
      "source": [
        "from gpt_train import calc_loss_loader\n",
        "\n",
        "torch.manual_seed(123) # 데이터 로더에서 셔플링을 하므로 재현성을 위해\n",
        "train_loss = calc_loss_loader(train_loader, gpt, device)\n",
        "val_loss = calc_loss_loader(val_loader, gpt, device)\n",
        "\n",
        "print(\"훈련 손실:\", train_loss)\n",
        "print(\"검증 손실:\", val_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4114399",
      "metadata": {
        "id": "f4114399"
      },
      "source": [
        "가장 큰 GPT-2 모델에서도 이를 반복할 수 있지만, 컨텍스트 길이를 업데이트하는 것을 잊지 마세요:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4a0c6d6",
      "metadata": {
        "id": "a4a0c6d6",
        "outputId": "66319ba2-73fb-47a5-d681-1b43f22ca5bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 164kiB/s]\n",
            "encoder.json: 100%|██████████| 1.04M/1.04M [00:01<00:00, 557kiB/s]\n",
            "hparams.json: 100%|██████████| 91.0/91.0 [00:00<00:00, 216kiB/s]\n",
            "model.ckpt.data-00000-of-00001: 100%|██████████| 6.23G/6.23G [33:49<00:00, 3.07MiB/s]\n",
            "model.ckpt.index: 100%|██████████| 20.7k/20.7k [00:00<00:00, 100kiB/s] \n",
            "model.ckpt.meta: 100%|██████████| 1.84M/1.84M [00:02<00:00, 738kiB/s] \n",
            "vocab.bpe: 100%|██████████| 456k/456k [00:01<00:00, 382kiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 손실: 3.3046506775750055\n",
            "검증 손실: 3.119534730911255\n"
          ]
        }
      ],
      "source": [
        "settings, params = download_and_load_gpt2(model_size=\"1558M\", models_dir=\"gpt2\")\n",
        "\n",
        "model_name = \"gpt2-xl (1558M)\"\n",
        "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
        "NEW_CONFIG.update(model_configs[model_name])\n",
        "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
        "\n",
        "gpt = GPTModel(NEW_CONFIG)\n",
        "gpt.eval()\n",
        "\n",
        "load_weights_into_gpt(gpt, params)\n",
        "gpt.to(device)\n",
        "\n",
        "torch.manual_seed(123)\n",
        "train_loss = calc_loss_loader(train_loader, gpt, device)\n",
        "val_loss = calc_loss_loader(val_loader, gpt, device)\n",
        "\n",
        "print(\"훈련 손실:\", train_loss)\n",
        "print(\"검증 손실:\", val_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "128b5db1",
      "metadata": {
        "id": "128b5db1"
      },
      "source": [
        "# 연습문제 5.6: 더 큰 모델 시도하기\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b697f493",
      "metadata": {
        "id": "b697f493"
      },
      "source": [
        "- 메인 챕터에서는 124M 매개변수만 있는 가장 작은 GPT-2 모델을 사용하여 실험했습니다.\n",
        "- 이유는 리소스 요구 사항을 최대한 낮추기 위해서였습니다.\n",
        "- 그러나 최소한의 코드 변경으로 더 큰 모델을 쉽게 실험할 수 있습니다.\n",
        "- 예를 들어, 5장에서 124M 모델 대신 1558M 모델을 로드하는 경우 변경해야 하는 코드는 두 줄뿐입니다.\n",
        "\n",
        "```python\n",
        "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")\n",
        "model_name = \"gpt2-small (124M)\"\n",
        "```\n",
        "\n",
        "- 업데이트된 코드는 다음과 같습니다.\n",
        "\n",
        "\n",
        "```python\n",
        "settings, params = download_and_load_gpt2(model_size=\"1558M\", models_dir=\"gpt2\")\n",
        "model_name = \"gpt2-xl (1558M)\"\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b99b82df",
      "metadata": {
        "id": "b99b82df"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "import torch\n",
        "from previous_chapters import GPTModel\n",
        "\n",
        "\n",
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,   # 어휘 크기\n",
        "    \"context_length\": 256, # 단축된 컨텍스트 길이 (원래: 1024)\n",
        "    \"emb_dim\": 768,        # 임베딩 차원\n",
        "    \"n_heads\": 12,         # 어텐션 헤드 수\n",
        "    \"n_layers\": 12,        # 층 수\n",
        "    \"drop_rate\": 0.1,      # 드롭아웃 비율\n",
        "    \"qkv_bias\": False      # 쿼리-키-값 편향\n",
        "}\n",
        "\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27b70ae2",
      "metadata": {
        "id": "27b70ae2",
        "outputId": "4890b50f-d130-4472-f77d-ed7500ea7568",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/1558M/checkpoint\n",
            "File already exists and is up-to-date: gpt2/1558M/encoder.json\n",
            "File already exists and is up-to-date: gpt2/1558M/hparams.json\n",
            "File already exists and is up-to-date: gpt2/1558M/model.ckpt.data-00000-of-00001\n",
            "File already exists and is up-to-date: gpt2/1558M/model.ckpt.index\n",
            "File already exists and is up-to-date: gpt2/1558M/model.ckpt.meta\n",
            "File already exists and is up-to-date: gpt2/1558M/vocab.bpe\n"
          ]
        }
      ],
      "source": [
        "from gpt_download import download_and_load_gpt2\n",
        "from gpt_generate import load_weights_into_gpt\n",
        "\n",
        "\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "model_name = \"gpt2-xl (1558M)\"\n",
        "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
        "NEW_CONFIG.update(model_configs[model_name])\n",
        "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
        "\n",
        "gpt = GPTModel(NEW_CONFIG)\n",
        "gpt.eval()\n",
        "\n",
        "settings, params = download_and_load_gpt2(model_size=\"1558M\", models_dir=\"gpt2\")\n",
        "load_weights_into_gpt(gpt, params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b5a9943",
      "metadata": {
        "id": "4b5a9943"
      },
      "outputs": [],
      "source": [
        "from gpt_generate import generate, text_to_token_ids, token_ids_to_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d14a5fe",
      "metadata": {
        "id": "6d14a5fe",
        "outputId": "501fd5a5-f886-4f4d-f57f-8408585b3611",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "출력 텍스트:\n",
            " Every effort moves you toward finding an ideal life. You don't have to accept your current one at once, because if you do you'll never\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "token_ids = generate(\n",
        "    model=gpt,\n",
        "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
        "    max_new_tokens=25,\n",
        "    context_size=NEW_CONFIG[\"context_length\"],\n",
        "    top_k=50,\n",
        "    temperature=1.5\n",
        ")\n",
        "\n",
        "print(\"출력 텍스트:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}